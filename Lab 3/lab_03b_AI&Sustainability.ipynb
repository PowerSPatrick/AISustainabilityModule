{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kT3yRnmIOnn"
   },
   "source": [
    "# Lab 03.b - Week 4\n",
    "# Time Series Processing\n",
    "## Using CWT and deep neural networks for a time series classification task\n",
    "\n",
    "In this task, you will be using accelerometer data. This is signal data used to determine what state someone is in, i.e walking, running etc. For this, we will be utilising CWT to help achieve accurate classification using some basic deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbmLodlXmZT4",
    "outputId": "4f380eee-906e-4982-aa81-bd9e1600cb04"
   },
   "outputs": [],
   "source": [
    "!pip install PyWavelets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kznp_exqHLZP",
    "outputId": "68e2ab1f-74f8-4caf-9b9b-31e50a566d44"
   },
   "outputs": [],
   "source": [
    "# All Includes\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pywt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "#import keras (high level API) wiht tensorflow as backend\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDfYMGHI9G4"
   },
   "source": [
    "The below code cells are util functions and cells to load the dataset for this lab. Please ensure that you have access to the data. The dataset is available on SurreyLearn so please downloaded it and adjust path variable as needed.\n",
    "\n",
    "*Note*: if you are using Google Colab please upload the dataset to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkX0L7xqHLZd"
   },
   "outputs": [],
   "source": [
    "def load_y_data(y_path):\n",
    "    y = np.loadtxt(y_path, dtype=np.int32).reshape(-1,1)\n",
    "    # change labels range from 1-6 t 0-5, this enables a sparse_categorical_crossentropy loss function\n",
    "    return y - 1\n",
    "\n",
    "def load_X_data(X_path):\n",
    "    X_signal_paths = [X_path + file for file in os.listdir(X_path)]\n",
    "    X_signals = [np.loadtxt(path, dtype=np.float32) for path in X_signal_paths]\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4BwDM5Vn7Md",
    "outputId": "b0f03e0e-9cee-4038-d006-97b25310ab75"
   },
   "outputs": [],
   "source": [
    "#Upload dataset to google drive if using google colab and mount\n",
    "#If using Jupyter ensure notebook has access and adjust path as needed\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rF4A6q5WHLZe",
    "outputId": "fe4eb7d9-fedf-4a2a-975c-4454723fa59b"
   },
   "outputs": [],
   "source": [
    "#adjust path as needed\n",
    "PATH = '/content/drive/MyDrive/UCI HAR Dataset/'\n",
    "LABEL_NAMES = [\"Walking\", \"Walking upstairs\", \"Walking downstairs\", \"Sitting\", \"Standing\", \"Laying\"]\n",
    "\n",
    "# load X data\n",
    "X_train = load_X_data(PATH + 'train/Inertial Signals/')\n",
    "X_test = load_X_data(PATH + 'test/Inertial Signals/')\n",
    "# load y label\n",
    "y_train = load_y_data(PATH + 'train/y_train.txt')\n",
    "y_test = load_y_data(PATH + 'test/y_test.txt')\n",
    "\n",
    "print(\"useful information:\")\n",
    "print(f\"shapes (n_samples, n_steps, n_signals) of X_train: {X_train.shape} and X_test: {X_test.shape}\")\n",
    "X_all = np.concatenate([X_train, X_test])\n",
    "print(f\"all X's have following mean: {format(X_all.mean(), '.2f')} and standard deviation: {format(X_all.std(), '.2f')} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpZUi-GNJ1zH"
   },
   "source": [
    "Here we split the labels of the dataset into a list for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpPyfdrTHLZj"
   },
   "outputs": [],
   "source": [
    "def split_indices_per_label(y):\n",
    "    indicies_per_label = [[] for x in range(0,6)]\n",
    "    # loop over the six labels\n",
    "    for i in range(6):\n",
    "        indicies_per_label[i] = np.where(y == i)[0]\n",
    "    return indicies_per_label\n",
    "\n",
    "# list of list of sample indicies per activity\n",
    "train_labels_indicies = split_indices_per_label(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqFwZyo-J_7O"
   },
   "source": [
    "Here we are performing CWT on the dataset and plotting an exaple of each class in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "xJsZ4ktWHLZk",
    "outputId": "9fd1fdfa-bdb9-4891-9327-66d126cf8af2"
   },
   "outputs": [],
   "source": [
    "def plot_cwt_coeffs_per_label(X, label_indicies, label_names, signal, sample, scales, wavelet):\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(12,5))\n",
    "\n",
    "    for ax, indices, name in zip(axs.flat, label_indicies, label_names):\n",
    "        coeffs, freqs = pywt.cwt(X[indices[sample],:, signal], scales, wavelet = wavelet)\n",
    "        ax.imshow(coeffs, cmap = 'coolwarm', aspect = 'auto')\n",
    "        ax.set_title(name)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_ylabel('Scale')\n",
    "        ax.set_xlabel('Time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "#signal indicies: 0 = body acc x, 1 = body acc y, 2 = body acc z, 3 = body gyro x, 4 = body gyro y, 5 = body gyro z, 6 = total acc x, 7 = total acc y, 8 = total acc z\n",
    "signal = 3 # signal index\n",
    "sample = 1 # sample index of each label indicies list\n",
    "scales = np.arange(1, 65) # range of scales\n",
    "wavelet = 'morl' # mother wavelet\n",
    "\n",
    "plot_cwt_coeffs_per_label(X_train, train_labels_indicies, LABEL_NAMES, signal, sample, scales, wavelet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "QqwmHGYSHLZl",
    "outputId": "5d48e188-c2e3-466f-e118-9970f22a30e9"
   },
   "outputs": [],
   "source": [
    "#signal indicies: 0 = body acc x, 1 = body acc y, 2 = body acc z, 3 = body gyro x, 4 = body gyro y, 5 = body gyro z, 6 = total acc x, 7 = total acc y, 8 = total acc z\n",
    "signal = 8 # signal index\n",
    "sample = 1 # sample index of each label indicies list\n",
    "scales = np.arange(1, 65) # range of scales\n",
    "wavelet = 'morl' # mother wavelet\n",
    "\n",
    "plot_cwt_coeffs_per_label(X_train, train_labels_indicies, LABEL_NAMES, signal, sample, scales, wavelet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIm5kvlaF8Uv"
   },
   "source": [
    "## Challenge 1\n",
    "\n",
    "Create a function that will iterate over the dataset to create cwt images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGyGAUrDF8Af"
   },
   "outputs": [],
   "source": [
    "def create_cwt_images(X, n_scales, rescale_size, wavelet_name = \"morl\"):\n",
    "    n_samples = X.shape[0]\n",
    "    n_signals = X.shape[2]\n",
    "\n",
    "    # range of scales from 1 to n_scales\n",
    "    scales = np.arange(1, n_scales + 1)\n",
    "\n",
    "    # pre allocate array\n",
    "    X_cwt = np.ndarray(shape=(n_samples, rescale_size, rescale_size, n_signals), dtype = 'float32')\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE###\n",
    "\n",
    "\n",
    "    return X_cwt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrx0gu2HLZn"
   },
   "outputs": [],
   "source": [
    "### RUN THE FUNCTION TO GENERATE IMAGES\n",
    "\n",
    "# amount of pixels in X and Y\n",
    "rescale_size = 64\n",
    "# determine the max scale size\n",
    "n_scales = 64\n",
    "\n",
    "X_train_cwt = create_cwt_images(X_train, n_scales, rescale_size)\n",
    "print(f\"shapes (n_samples, x_img, y_img, z_img) of X_train_cwt: {X_train_cwt.shape}\")\n",
    "X_test_cwt = create_cwt_images(X_test, n_scales, rescale_size)\n",
    "print(f\"shapes (n_samples, x_img, y_img, z_img) of X_test_cwt: {X_test_cwt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiqE0JTtKPoY"
   },
   "source": [
    "### Important practice\n",
    "\n",
    "When working with deep learning to any capacity, it is a good habit to keep track of all computational variables. This means things such as total training time, time per epoch, inference time etc. This is especially paramount when working with AI for sustainability as we need to consider computation as a resource. Therefore we want to always ensure we keep track of these values as good practice.\n",
    "\n",
    "Please observe how this is done in the first challenge, as you will need to apply this to challenge 3 yourself and all future labs and courseworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2G4__TwHbVg"
   },
   "source": [
    "## Challenge 2\n",
    "\n",
    "Challenge 2 is to build a basic Multi-Layer Perceptron that can try to classify some test images. Keep the model simple to start, and experiment with a few features such as dropout and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LukOjBqiJX1o"
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape, num_classes, activation='relu'):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPu_eHxrJl7G"
   },
   "source": [
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "\n",
    "  Remember a MLP is a neural network of stacked linear layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVdBhUSpHLZp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def compile_and_fit_model(model, X_train, y_train, X_test, y_test, batch_size, n_epochs):\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    # define callbacks\n",
    "    callbacks = [ModelCheckpoint(filepath='best_model.keras', monitor='val_sparse_categorical_accuracy', save_best_only=True)]\n",
    "\n",
    "    ### Here we are noting the starting time of training\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=n_epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(X_test, y_test))\n",
    "\n",
    "    ### And here, once the model finishes training we log the time and the difference is the time taken to train the model\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return model, history, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHtJ6hIvIPMW"
   },
   "outputs": [],
   "source": [
    "# shape of the input images\n",
    "###Set input shape to the appropriate dimensions\n",
    "input_shape = (X_train_cwt.shape[1], X_train_cwt.shape[2], X_train_cwt.shape[3])\n",
    "\n",
    "# create mlp model\n",
    "mlp_model = build_mlp_model(input_shape, 6, \"relu\")\n",
    "# train mlp model\n",
    "trained_mlp_model, mlp_history, mlp_training_time = compile_and_fit_model(mlp_model, X_train_cwt, y_train, X_test_cwt, y_test, 64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SOeD9uELVts"
   },
   "outputs": [],
   "source": [
    "### USE MODEL TO MAKE PREDICTIONS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4Qfwp29Lq3Q"
   },
   "source": [
    "Now that we have a trained model and have output predictions, we can evaluate the model.\n",
    "\n",
    "First, let us check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caM4Nc1cL2in"
   },
   "outputs": [],
   "source": [
    "# DETERMINE THE ACCURACY OF THE MODEL AND PRINT THE VALUE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf46kxdCR4iG"
   },
   "source": [
    "Then, lets plot the training and validation accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4nrnfaASHhG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mlp_history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
    "plt.plot(mlp_history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('MLP Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mlp_history.history['loss'], label='Training Loss')\n",
    "plt.plot(mlp_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('MLP Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCyHvam6MGeY"
   },
   "source": [
    "Next, let us generate a confusion matrix to further visualise the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucRdxi5DHLZr"
   },
   "outputs": [],
   "source": [
    "def create_confusion_matrix(y_pred, y_test):\n",
    "    #calculate the confusion matrix\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    ax.imshow(confmat, cmap=plt.cm.Blues, alpha=0.5)\n",
    "\n",
    "    n_labels = len(LABEL_NAMES)\n",
    "    ax.set_xticks(np.arange(n_labels))\n",
    "    ax.set_yticks(np.arange(n_labels))\n",
    "    ax.set_xticklabels(LABEL_NAMES)\n",
    "    ax.set_yticklabels(LABEL_NAMES)\n",
    "\n",
    "    # rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # loop over data dimensions and create text annotations.\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=i, y=j, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "    # avoid that the first and last row cut in half\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBJG_baGSOQ2"
   },
   "source": [
    "Let's also create a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToRjnYrwSXnW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM8eUsD3SaQO"
   },
   "source": [
    "And lastly, let us check the inference time of the model. We do this by perfomring inference over every index in test data and averaging the time to best represent the inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBQBFfaDSrHH"
   },
   "outputs": [],
   "source": [
    "inference_times = []\n",
    "# Perform inference and record times\n",
    "for i in range(len(X_test_cwt)):\n",
    "    start_time = time.time()\n",
    "    _ = trained_mlp_model.predict(np.array([X_test_cwt[i]]), verbose=0)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate average and standard deviation\n",
    "avg_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "print(f\"Training Time: {mlp_training_time} s\")\n",
    "print(f'Average Inference Time over {len(X_test_cwt)} runs: {avg_time*1000:.2f} ms')\n",
    "print(f'Standard Deviation: {std_time*1000:.2f} ms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXEU-xNmMkx2"
   },
   "source": [
    "## Challenge 3\n",
    "\n",
    "For challenge 3, instead of a multi-layer perceptron let's build a CNN model for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf-e-YMqHLZs"
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(activation, input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOiBPyNqMzAt"
   },
   "source": [
    "Next, let us build train the model in the same way we did for the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwBbHeSRHLZs"
   },
   "outputs": [],
   "source": [
    "###YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x9ENZGsN_89"
   },
   "source": [
    "Now that you have built and trained the model, let's evaluate the model like how we did in Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eptPbf0POcxt"
   },
   "outputs": [],
   "source": [
    "###MAKE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqFIvKGjMj3w"
   },
   "outputs": [],
   "source": [
    "###DETERMINE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIdTyuEcf0b9"
   },
   "outputs": [],
   "source": [
    "###PLOT ACCURACY AND LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO61OEDeMj6J"
   },
   "outputs": [],
   "source": [
    "###PLOT CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbMZUvSgfy8w"
   },
   "outputs": [],
   "source": [
    "###CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymMFbkKwf3-e"
   },
   "outputs": [],
   "source": [
    "###INFERENCE TIME ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9jkgKbfOgET"
   },
   "source": [
    "## Challenge 4 - extra\n",
    "\n",
    "Now that we have 2 models that have been built and trained on our dataset for this lab, how do they compare with eachother? Which would you use if you were deciding?\n",
    "\n",
    "In practice we often test more than 2 architectures, as an extra challenge you can try and build a model that is neither a MLP or CNN and see how it compares. Also, you can try to see how the same models would perform without any signal processing whatsoever, or with other types of signal processing techniques - such as Fourier Transform."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
