{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kT3yRnmIOnn"
   },
   "source": [
    "# Lab 03.b - Week 4\n",
    "# Time Series Processing\n",
    "## Using CWT and deep neural networks for a time series classification task\n",
    "\n",
    "In this task, you will be using accelerometer data. This is signal data used to determine what state someone is in, i.e walking, running etc. For this, we will be utilising CWT to help achieve accurate classification using some basic deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbmLodlXmZT4",
    "outputId": "4f380eee-906e-4982-aa81-bd9e1600cb04",
    "ExecuteTime": {
     "end_time": "2025-02-24T16:49:14.199628Z",
     "start_time": "2025-02-24T16:46:55.022009Z"
    }
   },
   "source": [
    "!pip install PyWavelets -q\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tensorflow\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-image\n",
    "!pip install xgboost\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Collecting pandas>=1.2 (from seaborn)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.5 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 11.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.5 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, seaborn\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 seaborn-0.13.2 tzdata-2025.1\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl.metadata (59 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\spatr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/390.3 MB 10.0 MB/s eta 0:00:39\n",
      "   ---------------------------------------- 4.2/390.3 MB 10.5 MB/s eta 0:00:37\n",
      "    --------------------------------------- 6.8/390.3 MB 11.0 MB/s eta 0:00:35\n",
      "    --------------------------------------- 9.4/390.3 MB 11.5 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 12.1/390.3 MB 11.6 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 14.7/390.3 MB 11.7 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 17.3/390.3 MB 11.7 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 19.9/390.3 MB 11.8 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 22.3/390.3 MB 11.8 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 24.9/390.3 MB 11.9 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 27.0/390.3 MB 11.8 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 29.4/390.3 MB 11.6 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 31.7/390.3 MB 11.6 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 34.1/390.3 MB 11.6 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 36.7/390.3 MB 11.6 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 39.6/390.3 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 41.9/390.3 MB 11.7 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 44.8/390.3 MB 11.8 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 47.4/390.3 MB 11.8 MB/s eta 0:00:30\n",
      "   ----- ---------------------------------- 50.1/390.3 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 53.0/390.3 MB 11.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 55.6/390.3 MB 12.0 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 57.9/390.3 MB 11.9 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 60.8/390.3 MB 12.0 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 63.4/390.3 MB 12.0 MB/s eta 0:00:28\n",
      "   ------ --------------------------------- 66.1/390.3 MB 12.1 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 68.4/390.3 MB 12.0 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 70.5/390.3 MB 11.9 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 73.4/390.3 MB 12.0 MB/s eta 0:00:27\n",
      "   ------- -------------------------------- 76.0/390.3 MB 12.0 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 78.4/390.3 MB 12.0 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 81.3/390.3 MB 12.0 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 83.6/390.3 MB 12.0 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 86.5/390.3 MB 12.0 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 89.1/390.3 MB 12.0 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 91.5/390.3 MB 12.0 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 93.8/390.3 MB 12.0 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 96.5/390.3 MB 12.0 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 98.8/390.3 MB 12.0 MB/s eta 0:00:25\n",
      "   ---------- ---------------------------- 101.7/390.3 MB 12.0 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 104.3/390.3 MB 12.0 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 107.0/390.3 MB 12.1 MB/s eta 0:00:24\n",
      "   ---------- ---------------------------- 109.8/390.3 MB 12.1 MB/s eta 0:00:24\n",
      "   ----------- --------------------------- 112.7/390.3 MB 12.1 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 115.6/390.3 MB 12.1 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 116.7/390.3 MB 12.0 MB/s eta 0:00:23\n",
      "   ----------- --------------------------- 119.0/390.3 MB 12.0 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 121.6/390.3 MB 12.0 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 124.5/390.3 MB 12.0 MB/s eta 0:00:23\n",
      "   ------------ -------------------------- 126.9/390.3 MB 12.0 MB/s eta 0:00:22\n",
      "   ------------ -------------------------- 129.8/390.3 MB 12.0 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 132.4/390.3 MB 12.0 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 135.0/390.3 MB 12.1 MB/s eta 0:00:22\n",
      "   ------------- ------------------------- 137.9/390.3 MB 12.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 141.0/390.3 MB 12.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 144.4/390.3 MB 12.2 MB/s eta 0:00:21\n",
      "   -------------- ------------------------ 147.3/390.3 MB 12.2 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 150.2/390.3 MB 12.2 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 153.1/390.3 MB 12.3 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 155.7/390.3 MB 12.3 MB/s eta 0:00:20\n",
      "   --------------- ----------------------- 158.3/390.3 MB 12.3 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 161.2/390.3 MB 12.3 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 163.6/390.3 MB 12.3 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 166.2/390.3 MB 12.3 MB/s eta 0:00:19\n",
      "   ---------------- ---------------------- 169.1/390.3 MB 12.3 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 172.0/390.3 MB 12.3 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 174.9/390.3 MB 12.3 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 177.5/390.3 MB 12.3 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 180.4/390.3 MB 12.4 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 183.0/390.3 MB 12.4 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 185.3/390.3 MB 12.4 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 187.7/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 190.3/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 192.7/390.3 MB 12.3 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 195.0/390.3 MB 12.3 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 196.3/390.3 MB 12.2 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 197.1/390.3 MB 12.1 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 200.0/390.3 MB 12.1 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 202.9/390.3 MB 12.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 206.0/390.3 MB 12.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 208.7/390.3 MB 12.2 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 211.6/390.3 MB 12.2 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 214.4/390.3 MB 12.2 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 217.8/390.3 MB 12.3 MB/s eta 0:00:15\n",
      "   ---------------------- ---------------- 220.7/390.3 MB 12.3 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 223.3/390.3 MB 12.3 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 226.2/390.3 MB 12.3 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 228.9/390.3 MB 12.3 MB/s eta 0:00:14\n",
      "   ----------------------- --------------- 231.2/390.3 MB 12.3 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 233.8/390.3 MB 12.3 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 236.7/390.3 MB 12.3 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 239.6/390.3 MB 12.3 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 241.2/390.3 MB 12.3 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 243.8/390.3 MB 12.3 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 246.7/390.3 MB 12.3 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 249.6/390.3 MB 12.3 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 252.4/390.3 MB 12.3 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 255.3/390.3 MB 12.3 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 258.2/390.3 MB 12.3 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 260.8/390.3 MB 12.3 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 263.5/390.3 MB 12.4 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 266.3/390.3 MB 12.4 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 269.2/390.3 MB 12.4 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 272.1/390.3 MB 12.4 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 274.7/390.3 MB 12.4 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 277.6/390.3 MB 12.4 MB/s eta 0:00:10\n",
      "   ---------------------------- ---------- 280.8/390.3 MB 12.5 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 283.6/390.3 MB 12.5 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 286.3/390.3 MB 12.5 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 289.1/390.3 MB 12.5 MB/s eta 0:00:09\n",
      "   ----------------------------- --------- 291.2/390.3 MB 12.5 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 292.3/390.3 MB 12.5 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 294.9/390.3 MB 12.5 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 297.8/390.3 MB 12.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 300.9/390.3 MB 12.5 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 304.1/390.3 MB 12.5 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 307.0/390.3 MB 12.5 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 309.9/390.3 MB 12.5 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 312.7/390.3 MB 12.6 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 315.9/390.3 MB 12.6 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 319.0/390.3 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 321.9/390.3 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 325.1/390.3 MB 12.6 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 327.7/390.3 MB 12.6 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 330.8/390.3 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.7/390.3 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 336.6/390.3 MB 12.7 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 339.7/390.3 MB 12.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 342.4/390.3 MB 12.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 344.7/390.3 MB 12.7 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 347.6/390.3 MB 12.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 350.5/390.3 MB 12.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 352.8/390.3 MB 12.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 355.7/390.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 358.9/390.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.5/390.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 363.9/390.3 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 366.5/390.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 369.1/390.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.5/390.3 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 373.0/390.3 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 375.9/390.3 MB 12.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 378.8/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.7/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.0/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  386.9/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.5/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.3/390.3 MB 12.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.4/4.3 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading h5py-3.13.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 13.3 MB/s eta 0:00:00\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 13.2 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.6/26.4 MB 12.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 5.2/26.4 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.4/26.4 MB 13.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.7/26.4 MB 12.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.6/26.4 MB 12.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.5/26.4 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 19.1/26.4 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.8/26.4 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.6/26.4 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Downloading numpy-2.0.2-cp312-cp312-win_amd64.whl (15.6 MB)\n",
      "   ---------------------------------------- 0.0/15.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.6/15.6 MB 13.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.2/15.6 MB 12.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.9/15.6 MB 12.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.5/15.6 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.4/15.6 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.5/15.6 MB 12.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.6/15.6 MB 12.1 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 2.9/5.5 MB 14.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 12.0 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading wrapt-1.17.2-cp312-cp312-win_amd64.whl (38 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, astunparse, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.8.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.0 protobuf-5.29.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\spatr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\spatr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kznp_exqHLZP",
    "outputId": "68e2ab1f-74f8-4caf-9b9b-31e50a566d44",
    "ExecuteTime": {
     "end_time": "2025-02-24T17:28:48.619395Z",
     "start_time": "2025-02-24T17:28:46.074445Z"
    }
   },
   "source": [
    "# All Includes\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pywt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "#import keras (high level API) wiht tensorflow as backend\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWDfYMGHI9G4"
   },
   "source": [
    "The below code cells are util functions and cells to load the dataset for this lab. Please ensure that you have access to the data. The dataset is available on SurreyLearn so please downloaded it and adjust path variable as needed.\n",
    "\n",
    "*Note*: if you are using Google Colab please upload the dataset to your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GkX0L7xqHLZd",
    "ExecuteTime": {
     "end_time": "2025-02-24T17:28:50.931701Z",
     "start_time": "2025-02-24T17:28:50.926477Z"
    }
   },
   "source": [
    "def load_y_data(y_path):\n",
    "    y = np.loadtxt(y_path, dtype=np.int32).reshape(-1,1)\n",
    "    # change labels range from 1-6 t 0-5, this enables a sparse_categorical_crossentropy loss function\n",
    "    return y - 1\n",
    "\n",
    "def load_X_data(X_path):\n",
    "    X_signal_paths = [X_path + file for file in os.listdir(X_path)]\n",
    "    X_signals = [np.loadtxt(path, dtype=np.float32) for path in X_signal_paths]\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4BwDM5Vn7Md",
    "outputId": "b0f03e0e-9cee-4038-d006-97b25310ab75"
   },
   "outputs": [],
   "source": [
    "#Upload dataset to google drive if using google colab and mount\n",
    "#If using Jupyter ensure notebook has access and adjust path as needed\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rF4A6q5WHLZe",
    "outputId": "fe4eb7d9-fedf-4a2a-975c-4454723fa59b"
   },
   "outputs": [],
   "source": [
    "#adjust path as needed\n",
    "PATH = '/content/drive/MyDrive/UCI HAR Dataset/'\n",
    "LABEL_NAMES = [\"Walking\", \"Walking upstairs\", \"Walking downstairs\", \"Sitting\", \"Standing\", \"Laying\"]\n",
    "\n",
    "# load X data\n",
    "X_train = load_X_data(PATH + 'train/Inertial Signals/')\n",
    "X_test = load_X_data(PATH + 'test/Inertial Signals/')\n",
    "# load y label\n",
    "y_train = load_y_data(PATH + 'train/y_train.txt')\n",
    "y_test = load_y_data(PATH + 'test/y_test.txt')\n",
    "\n",
    "print(\"useful information:\")\n",
    "print(f\"shapes (n_samples, n_steps, n_signals) of X_train: {X_train.shape} and X_test: {X_test.shape}\")\n",
    "X_all = np.concatenate([X_train, X_test])\n",
    "print(f\"all X's have following mean: {format(X_all.mean(), '.2f')} and standard deviation: {format(X_all.std(), '.2f')} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpZUi-GNJ1zH"
   },
   "source": [
    "Here we split the labels of the dataset into a list for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpPyfdrTHLZj"
   },
   "outputs": [],
   "source": [
    "def split_indices_per_label(y):\n",
    "    indicies_per_label = [[] for x in range(0,6)]\n",
    "    # loop over the six labels\n",
    "    for i in range(6):\n",
    "        indicies_per_label[i] = np.where(y == i)[0]\n",
    "    return indicies_per_label\n",
    "\n",
    "# list of list of sample indicies per activity\n",
    "train_labels_indicies = split_indices_per_label(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqFwZyo-J_7O"
   },
   "source": [
    "Here we are performing CWT on the dataset and plotting an exaple of each class in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "xJsZ4ktWHLZk",
    "outputId": "9fd1fdfa-bdb9-4891-9327-66d126cf8af2"
   },
   "outputs": [],
   "source": [
    "def plot_cwt_coeffs_per_label(X, label_indicies, label_names, signal, sample, scales, wavelet):\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(12,5))\n",
    "\n",
    "    for ax, indices, name in zip(axs.flat, label_indicies, label_names):\n",
    "        coeffs, freqs = pywt.cwt(X[indices[sample],:, signal], scales, wavelet = wavelet)\n",
    "        ax.imshow(coeffs, cmap = 'coolwarm', aspect = 'auto')\n",
    "        ax.set_title(name)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.set_ylabel('Scale')\n",
    "        ax.set_xlabel('Time')\n",
    "    plt.tight_layout()\n",
    "\n",
    "#signal indicies: 0 = body acc x, 1 = body acc y, 2 = body acc z, 3 = body gyro x, 4 = body gyro y, 5 = body gyro z, 6 = total acc x, 7 = total acc y, 8 = total acc z\n",
    "signal = 3 # signal index\n",
    "sample = 1 # sample index of each label indicies list\n",
    "scales = np.arange(1, 65) # range of scales\n",
    "wavelet = 'morl' # mother wavelet\n",
    "\n",
    "plot_cwt_coeffs_per_label(X_train, train_labels_indicies, LABEL_NAMES, signal, sample, scales, wavelet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "QqwmHGYSHLZl",
    "outputId": "5d48e188-c2e3-466f-e118-9970f22a30e9"
   },
   "outputs": [],
   "source": [
    "#signal indicies: 0 = body acc x, 1 = body acc y, 2 = body acc z, 3 = body gyro x, 4 = body gyro y, 5 = body gyro z, 6 = total acc x, 7 = total acc y, 8 = total acc z\n",
    "signal = 8 # signal index\n",
    "sample = 1 # sample index of each label indicies list\n",
    "scales = np.arange(1, 65) # range of scales\n",
    "wavelet = 'morl' # mother wavelet\n",
    "\n",
    "plot_cwt_coeffs_per_label(X_train, train_labels_indicies, LABEL_NAMES, signal, sample, scales, wavelet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIm5kvlaF8Uv"
   },
   "source": [
    "## Challenge 1\n",
    "\n",
    "Create a function that will iterate over the dataset to create cwt images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGyGAUrDF8Af"
   },
   "outputs": [],
   "source": [
    "def create_cwt_images(X, n_scales, rescale_size, wavelet_name = \"morl\"):\n",
    "    n_samples = X.shape[0]\n",
    "    n_signals = X.shape[2]\n",
    "\n",
    "    # range of scales from 1 to n_scales\n",
    "    scales = np.arange(1, n_scales + 1)\n",
    "\n",
    "    # pre allocate array\n",
    "    X_cwt = np.ndarray(shape=(n_samples, rescale_size, rescale_size, n_signals), dtype = 'float32')\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE###\n",
    "\n",
    "\n",
    "    return X_cwt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crrx0gu2HLZn"
   },
   "outputs": [],
   "source": [
    "### RUN THE FUNCTION TO GENERATE IMAGES\n",
    "\n",
    "# amount of pixels in X and Y\n",
    "rescale_size = 64\n",
    "# determine the max scale size\n",
    "n_scales = 64\n",
    "\n",
    "X_train_cwt = create_cwt_images(X_train, n_scales, rescale_size)\n",
    "print(f\"shapes (n_samples, x_img, y_img, z_img) of X_train_cwt: {X_train_cwt.shape}\")\n",
    "X_test_cwt = create_cwt_images(X_test, n_scales, rescale_size)\n",
    "print(f\"shapes (n_samples, x_img, y_img, z_img) of X_test_cwt: {X_test_cwt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiqE0JTtKPoY"
   },
   "source": [
    "### Important practice\n",
    "\n",
    "When working with deep learning to any capacity, it is a good habit to keep track of all computational variables. This means things such as total training time, time per epoch, inference time etc. This is especially paramount when working with AI for sustainability as we need to consider computation as a resource. Therefore we want to always ensure we keep track of these values as good practice.\n",
    "\n",
    "Please observe how this is done in the first challenge, as you will need to apply this to challenge 3 yourself and all future labs and courseworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2G4__TwHbVg"
   },
   "source": [
    "## Challenge 2\n",
    "\n",
    "Challenge 2 is to build a basic Multi-Layer Perceptron that can try to classify some test images. Keep the model simple to start, and experiment with a few features such as dropout and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LukOjBqiJX1o"
   },
   "outputs": [],
   "source": [
    "def build_mlp_model(input_shape, num_classes, activation='relu'):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPu_eHxrJl7G"
   },
   "source": [
    "<details>\n",
    "  <summary>Hint</summary>\n",
    "\n",
    "  Remember a MLP is a neural network of stacked linear layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVdBhUSpHLZp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def compile_and_fit_model(model, X_train, y_train, X_test, y_test, batch_size, n_epochs):\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    # define callbacks\n",
    "    callbacks = [ModelCheckpoint(filepath='best_model.keras', monitor='val_sparse_categorical_accuracy', save_best_only=True)]\n",
    "\n",
    "    ### Here we are noting the starting time of training\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(x=X_train,\n",
    "                        y=y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=n_epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(X_test, y_test))\n",
    "\n",
    "    ### And here, once the model finishes training we log the time and the difference is the time taken to train the model\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    return model, history, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHtJ6hIvIPMW"
   },
   "outputs": [],
   "source": [
    "# shape of the input images\n",
    "###Set input shape to the appropriate dimensions\n",
    "input_shape = (X_train_cwt.shape[1], X_train_cwt.shape[2], X_train_cwt.shape[3])\n",
    "\n",
    "# create mlp model\n",
    "mlp_model = build_mlp_model(input_shape, 6, \"relu\")\n",
    "# train mlp model\n",
    "trained_mlp_model, mlp_history, mlp_training_time = compile_and_fit_model(mlp_model, X_train_cwt, y_train, X_test_cwt, y_test, 64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SOeD9uELVts"
   },
   "outputs": [],
   "source": [
    "### USE MODEL TO MAKE PREDICTIONS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4Qfwp29Lq3Q"
   },
   "source": [
    "Now that we have a trained model and have output predictions, we can evaluate the model.\n",
    "\n",
    "First, let us check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "caM4Nc1cL2in"
   },
   "outputs": [],
   "source": [
    "# DETERMINE THE ACCURACY OF THE MODEL AND PRINT THE VALUE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf46kxdCR4iG"
   },
   "source": [
    "Then, lets plot the training and validation accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4nrnfaASHhG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the training and validation accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mlp_history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
    "plt.plot(mlp_history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('MLP Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mlp_history.history['loss'], label='Training Loss')\n",
    "plt.plot(mlp_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('MLP Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCyHvam6MGeY"
   },
   "source": [
    "Next, let us generate a confusion matrix to further visualise the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucRdxi5DHLZr"
   },
   "outputs": [],
   "source": [
    "def create_confusion_matrix(y_pred, y_test):\n",
    "    #calculate the confusion matrix\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    ax.imshow(confmat, cmap=plt.cm.Blues, alpha=0.5)\n",
    "\n",
    "    n_labels = len(LABEL_NAMES)\n",
    "    ax.set_xticks(np.arange(n_labels))\n",
    "    ax.set_yticks(np.arange(n_labels))\n",
    "    ax.set_xticklabels(LABEL_NAMES)\n",
    "    ax.set_yticklabels(LABEL_NAMES)\n",
    "\n",
    "    # rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # loop over data dimensions and create text annotations.\n",
    "    for i in range(confmat.shape[0]):\n",
    "        for j in range(confmat.shape[1]):\n",
    "            ax.text(x=i, y=j, s=confmat[i, j], va='center', ha='center')\n",
    "\n",
    "    # avoid that the first and last row cut in half\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBJG_baGSOQ2"
   },
   "source": [
    "Let's also create a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToRjnYrwSXnW"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM8eUsD3SaQO"
   },
   "source": [
    "And lastly, let us check the inference time of the model. We do this by perfomring inference over every index in test data and averaging the time to best represent the inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBQBFfaDSrHH"
   },
   "outputs": [],
   "source": [
    "inference_times = []\n",
    "# Perform inference and record times\n",
    "for i in range(len(X_test_cwt)):\n",
    "    start_time = time.time()\n",
    "    _ = trained_mlp_model.predict(np.array([X_test_cwt[i]]), verbose=0)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate average and standard deviation\n",
    "avg_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "print(f\"Training Time: {mlp_training_time} s\")\n",
    "print(f'Average Inference Time over {len(X_test_cwt)} runs: {avg_time*1000:.2f} ms')\n",
    "print(f'Standard Deviation: {std_time*1000:.2f} ms')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXEU-xNmMkx2"
   },
   "source": [
    "## Challenge 3\n",
    "\n",
    "For challenge 3, instead of a multi-layer perceptron let's build a CNN model for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf-e-YMqHLZs"
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(activation, input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###YOUR CODE HERE###\n",
    "\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOiBPyNqMzAt"
   },
   "source": [
    "Next, let us build train the model in the same way we did for the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwBbHeSRHLZs"
   },
   "outputs": [],
   "source": [
    "###YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x9ENZGsN_89"
   },
   "source": [
    "Now that you have built and trained the model, let's evaluate the model like how we did in Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eptPbf0POcxt"
   },
   "outputs": [],
   "source": [
    "###MAKE PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqFIvKGjMj3w"
   },
   "outputs": [],
   "source": [
    "###DETERMINE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIdTyuEcf0b9"
   },
   "outputs": [],
   "source": [
    "###PLOT ACCURACY AND LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO61OEDeMj6J"
   },
   "outputs": [],
   "source": [
    "###PLOT CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbMZUvSgfy8w"
   },
   "outputs": [],
   "source": [
    "###CLASSIFICATION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymMFbkKwf3-e"
   },
   "outputs": [],
   "source": [
    "###INFERENCE TIME ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9jkgKbfOgET"
   },
   "source": [
    "## Challenge 4 - extra\n",
    "\n",
    "Now that we have 2 models that have been built and trained on our dataset for this lab, how do they compare with eachother? Which would you use if you were deciding?\n",
    "\n",
    "In practice we often test more than 2 architectures, as an extra challenge you can try and build a model that is neither a MLP or CNN and see how it compares. Also, you can try to see how the same models would perform without any signal processing whatsoever, or with other types of signal processing techniques - such as Fourier Transform."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
