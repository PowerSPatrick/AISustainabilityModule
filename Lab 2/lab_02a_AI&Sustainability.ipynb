{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETMI93kcUveq"
   },
   "source": [
    "# Lab 02.a - Week 3\n",
    "## Early Fire Detection with Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QnkeXhTU1O6"
   },
   "source": [
    "## **Challenge 01**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPFY7O47U2cr"
   },
   "source": [
    "First, let's load the saved NumPy array data from your previous lab and complete the sections marked with `TODO X` comments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0kx2BhSBVA0o",
    "ExecuteTime": {
     "end_time": "2025-02-19T17:45:32.227094Z",
     "start_time": "2025-02-19T17:45:32.048461Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.saving.saved_model.load import layers_module\n",
    "\n",
    "# TODO 01  - Load the data from LAB 01\n",
    "\n",
    "save_path = '../Lab1/processed_data/'\n",
    "X = np.load(save_path + 'X.npy')\n",
    "X_test = np.load(save_path + 'X_test.npy')\n",
    "y = np.load(save_path + 'y.npy')\n",
    "y_test = np.load(save_path + 'y_test.npy')\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JlrVSfzrPac"
   },
   "source": [
    "## **Challenge 02**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEWOXfLNrQXu"
   },
   "source": [
    "Now, let’s build our multi-layer perceptronmodel to detect wildfires!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "fVLP85ZarnFC",
    "outputId": "cc0b6e4f-a530-49fe-a199-b349996d8045",
    "ExecuteTime": {
     "end_time": "2025-02-19T18:06:47.515013Z",
     "start_time": "2025-02-19T18:06:47.486038Z"
    }
   },
   "source": [
    "#TODO 01 - Let’s begin with a simple Multi-Layer Perceptron (MLP) model.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "input_shape = [100,100,3]\n",
    "\n",
    "model = Sequential([\n",
    "    Input(input_shape),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Use a single output neuron with a sigmoid activation function for binary classification.\n",
    "])\n",
    "\n",
    "# Display the model architecture to visualise its layers and parameters.\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_9 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 30001     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 30,003\n",
      "Trainable params: 30,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MV_c4axornLy",
    "ExecuteTime": {
     "end_time": "2025-02-19T18:06:50.995282Z",
     "start_time": "2025-02-19T18:06:50.989341Z"
    }
   },
   "source": [
    "#TODO 02 - Let’s compile our MLP model with an appropriate loss function, optimizer, and evaluation metrics.\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam_v2.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',  # Use a loss function compatible with binary classification problems.\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hto2r8q8rnST",
    "outputId": "bf17725c-1cdc-4008-b560-269997af758f",
    "ExecuteTime": {
     "end_time": "2025-02-19T18:09:35.988371Z",
     "start_time": "2025-02-19T18:09:35.952835Z"
    }
   },
   "source": [
    "#TODO 03 - Let’s train our MLP model using the training data while monitoring its performance on the validation set.\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X, y, epochs=10, batch_size=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"Training Time (s): \", training_time)"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m      3\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m----> 5\u001B[0m history \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfit(X, y, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, validation_split\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      7\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      8\u001B[0m training_time \u001B[38;5;241m=\u001B[39m end_time \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/miniconda3/envs/AISustainabilityModule/lib/python3.12/site-packages/tensorflow/python/keras/engine/training.py:1137\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1131\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cluster_coordinator \u001B[38;5;241m=\u001B[39m cluster_coordinator\u001B[38;5;241m.\u001B[39mClusterCoordinator(\n\u001B[1;32m   1132\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy)\n\u001B[1;32m   1134\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistribute_strategy\u001B[38;5;241m.\u001B[39mscope(), \\\n\u001B[1;32m   1135\u001B[0m      training_utils\u001B[38;5;241m.\u001B[39mRespectCompiledTrainableState(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1136\u001B[0m   \u001B[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001B[39;00m\n\u001B[0;32m-> 1137\u001B[0m   data_handler \u001B[38;5;241m=\u001B[39m data_adapter\u001B[38;5;241m.\u001B[39mget_data_handler(\n\u001B[1;32m   1138\u001B[0m       x\u001B[38;5;241m=\u001B[39mx,\n\u001B[1;32m   1139\u001B[0m       y\u001B[38;5;241m=\u001B[39my,\n\u001B[1;32m   1140\u001B[0m       sample_weight\u001B[38;5;241m=\u001B[39msample_weight,\n\u001B[1;32m   1141\u001B[0m       batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1142\u001B[0m       steps_per_epoch\u001B[38;5;241m=\u001B[39msteps_per_epoch,\n\u001B[1;32m   1143\u001B[0m       initial_epoch\u001B[38;5;241m=\u001B[39minitial_epoch,\n\u001B[1;32m   1144\u001B[0m       epochs\u001B[38;5;241m=\u001B[39mepochs,\n\u001B[1;32m   1145\u001B[0m       shuffle\u001B[38;5;241m=\u001B[39mshuffle,\n\u001B[1;32m   1146\u001B[0m       class_weight\u001B[38;5;241m=\u001B[39mclass_weight,\n\u001B[1;32m   1147\u001B[0m       max_queue_size\u001B[38;5;241m=\u001B[39mmax_queue_size,\n\u001B[1;32m   1148\u001B[0m       workers\u001B[38;5;241m=\u001B[39mworkers,\n\u001B[1;32m   1149\u001B[0m       use_multiprocessing\u001B[38;5;241m=\u001B[39muse_multiprocessing,\n\u001B[1;32m   1150\u001B[0m       model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1151\u001B[0m       steps_per_execution\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution)\n\u001B[1;32m   1153\u001B[0m   \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[1;32m   1154\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[0;32m~/miniconda3/envs/AISustainabilityModule/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1397\u001B[0m, in \u001B[0;36mget_data_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1396\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 1397\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/AISustainabilityModule/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1151\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[1;32m   1148\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution \u001B[38;5;241m=\u001B[39m steps_per_execution\n\u001B[1;32m   1149\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution_value \u001B[38;5;241m=\u001B[39m steps_per_execution\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m-> 1151\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m select_data_adapter(x, y)\n\u001B[1;32m   1152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m adapter_cls(\n\u001B[1;32m   1153\u001B[0m     x,\n\u001B[1;32m   1154\u001B[0m     y,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1163\u001B[0m     distribution_strategy\u001B[38;5;241m=\u001B[39mdistribute_lib\u001B[38;5;241m.\u001B[39mget_strategy(),\n\u001B[1;32m   1164\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   1166\u001B[0m strategy \u001B[38;5;241m=\u001B[39m distribute_lib\u001B[38;5;241m.\u001B[39mget_strategy()\n",
      "File \u001B[0;32m~/miniconda3/envs/AISustainabilityModule/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:987\u001B[0m, in \u001B[0;36mselect_data_adapter\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect_data_adapter\u001B[39m(x, y):\n\u001B[1;32m    986\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m   adapter_cls \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mcls\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01min\u001B[39;00m ALL_ADAPTER_CLS \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mcan_handle(x, y)]\n\u001B[1;32m    988\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m adapter_cls:\n\u001B[1;32m    989\u001B[0m     \u001B[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001B[39;00m\n\u001B[1;32m    990\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to find data adapter that can handle \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    993\u001B[0m             _type_name(x), _type_name(y)))\n",
      "File \u001B[0;32m~/miniconda3/envs/AISustainabilityModule/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:706\u001B[0m, in \u001B[0;36mDatasetAdapter.can_handle\u001B[0;34m(x, y)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcan_handle\u001B[39m(x, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    705\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(x, (data_types\u001B[38;5;241m.\u001B[39mDatasetV1, data_types\u001B[38;5;241m.\u001B[39mDatasetV2)) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m--> 706\u001B[0m           _is_distributed_dataset(x))\n",
      "File \u001B[0;32m~/miniconda3/envs/AISustainabilityModule/lib/python3.12/site-packages/tensorflow/python/keras/engine/data_adapter.py:1696\u001B[0m, in \u001B[0;36m_is_distributed_dataset\u001B[0;34m(ds)\u001B[0m\n\u001B[1;32m   1695\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_is_distributed_dataset\u001B[39m(ds):\n\u001B[0;32m-> 1696\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ds, input_lib\u001B[38;5;241m.\u001B[39mDistributedDatasetInterface)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXzptcYrlCW3",
    "outputId": "cdad6657-8fe0-4405-a380-2fd4e25e8ee8"
   },
   "outputs": [],
   "source": [
    "#TODO 04 - Evaluate the MLP model on the test data to assess its performance.\n",
    "model.evaluate(..., ...) # Evaluate the model using data that it has never seen before to ensure an unbiased assessment of its generalisation ability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1Vai1AHwGnY"
   },
   "source": [
    "Let’s visualise the training and validation curves to assess the model's performance. Analyse the curves to determine if there is overfitting or underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "137heBD8v-3r",
    "outputId": "420d3bec-2873-4af4-dbb8-2ee70554f1b7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Plot_acc_loss(history):\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs_range = range(len(loss))\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(epochs_range, loss, label='Training Loss')\n",
    "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.show()\n",
    "Plot_acc_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TTwgiICv1S7"
   },
   "source": [
    "Now, let’s evaluate our model using classification metrics such as accuracy, precision, recall, and F1-score to gain deeper insights into its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZMepS2OGxGX8",
    "outputId": "259e3aa6-d2ea-4063-d3ae-95c8390e1140"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "pred = model.predict(X_test)\n",
    "binary_predictions = [1 if p > 0.5 else 0 for p in pred]\n",
    "\n",
    "\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GWzTT4UzSJN"
   },
   "source": [
    "And let’s analyse the confusion matrix to understand the model's performance in correctly and incorrectly classifying each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "jC748ivPyimS",
    "outputId": "42f997fe-322f-48e8-d265-a628a3290e82"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "#TODO 05 - Complete the function to calculate and display the classification metrics and the confusion matrix for the model's predictions.\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "  cm = confusion_matrix(..., ...)\n",
    "\n",
    "  # Define class labels\n",
    "  class_labels = [..., ...]\n",
    "\n",
    "  # Create a DataFrame\n",
    "  cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "\n",
    "  plt.figure(figsize=(8,6))\n",
    "  sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "  plt.ylabel('Actual')\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "plot_confusion_matrix(..., ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c1p72zNz_Dc"
   },
   "source": [
    "Finally, let’s visualise the images along with their predicted and true labels to better understand the model’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "hpQ49L7IwNWH",
    "outputId": "a13b08a4-bc4a-46bc-9000-d423b11c2e4b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "reverse_label_map = {\n",
    "    0:\"Fire\",1:\"Non_Fire\"\n",
    "}\n",
    "def view_grid_figures(predictions):\n",
    "  combined_list = list(zip(X_test, predictions))\n",
    "  Samples = random.sample(combined_list, 16)\n",
    "\n",
    "  total_images = 16\n",
    "  grid_size = (4,4)\n",
    "\n",
    "  fig, axes = plt.subplots(grid_size[0], grid_size[1])\n",
    "  axes = axes.flatten()\n",
    "\n",
    "  for idx in range(total_images):\n",
    "      ax = axes[idx]\n",
    "      ax.axis('off')\n",
    "\n",
    "\n",
    "      img = Samples[idx][0]\n",
    "      label = Samples[idx][1]\n",
    "      ax.imshow(img)\n",
    "\n",
    "      ax.set_title(reverse_label_map[label], fontsize=10)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "view_grid_figures(binary_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0d5svqbuFVx"
   },
   "source": [
    "Analysing Model Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJ83sKkNuVBQ",
    "outputId": "684d18bd-c2dd-4811-ff3c-b60eb7585790"
   },
   "outputs": [],
   "source": [
    "inference_times = []\n",
    "\n",
    "# Perform inference and record times\n",
    "for i in range(len(X_test)):\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(np.array([X_test[i,:,:,:]]), verbose=0)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "# Calculate average and standard deviation\n",
    "avg_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "\n",
    "print(f\"Training Time: {training_time} s\")\n",
    "print(f'Average Inference Time over {len(X_test)} runs: {avg_time*1000:.2f} ms')\n",
    "print(f'Standard Deviation: {std_time*1000:.2f} ms')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
